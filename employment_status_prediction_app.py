# -*- coding: utf-8 -*-
"""Employment Status Prediction App

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UTVED5jtZLoKZlDJsKiPkT2bPteuva4p
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler, LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings('ignore')

# --- Configuration and File Paths ---
DATA_FILE = 'train.csv'
MODEL_FILE = 'logistic_model.pkl'
SCALER_FILE = 'scaler.pkl'
COLUMNS_FILE = 'X_columns.pkl'

# --- Preprocessing Functions (Replicated from Notebook) ---

@st.cache_data # Cache data loading and preprocessing for performance
def load_and_preprocess_data(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        st.error(f"Error: '{file_path}' not found. Using dummy data for demonstration.")
        # Create dummy DataFrame for demonstration if file not found
        data = {
            'Person_id': [f'Id_test{i}' for i in range(4020)],
            'Survey_date': [f'2022-01-{i%28 + 1}' for i in range(4020)],
            'Round': np.random.randint(1, 5, 4020),
            'Status': np.random.choice(['studying', 'unemployed', 'wage employed', 'other', 'self employed', 'employment programme', 'wage and self employed'], 4020),
            'Tenure': np.random.randint(0, 3600, 4020).astype(float),
            'Geography': np.random.choice(['Rural', 'Urban', 'Suburb'], 4020),
            'Province': np.random.choice(['Gauteng', 'KwaZulu-Natal', 'Eastern Cape', 'Western Cape', 'Mpumalanga', 'North West', 'Free State', 'Limpopo', 'Northern Cape'], 4020),
            'Matric': np.random.choice([0.0, 1.0], 4020, p=[0.3, 0.5, 0.2]),
            'Degree': np.random.choice([0.0, 1.0], 4020, p=[0.4, 0.4, 0.2]),
            'Diploma': np.random.choice([0.0, 1.0], 4020, p=[0.4, 0.4, 0.2]),
            'Schoolquintile': np.random.choice([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], 4020, p=[0.05, 0.1, 0.15, 0.2, 0.2, 0.1, 0.2]),
            'Math': np.random.choice(['0 - 29 %', '30 - 39 %', '40 - 49 %', '50 - 59 %', '60 - 69 %', '70 - 79 %', '80 - 89 %', '90 - 100 %'], 4020, p=[0.1, 0.1, 0.15, 0.15, 0.1, 0.1, 0.05, 0.05, 0.2]),
            'Mathlit': np.random.choice(['0 - 29 %', '30 - 39 %', '40 - 49 %', '50 - 59 %', '60 - 69 %'], 4020, p=[0.1, 0.1, 0.2, 0.2, 0.1, 0.3]),
            'Additional_lang': np.random.choice(['0 - 29 %', '30 - 39 %', '40 - 49 %', '50 - 59 %', '60 - 69 %'], 4020, p=[0.1, 0.1, 0.2, 0.2, 0.1, 0.3]),
            'Home_lang': np.random.choice(['0 - 29 %', '30 - 39 %', '40 - 49 %'], 4020, p=[0.1, 0.1, 0.1, 0.7]),
            'Science': np.random.choice(['0 - 29 %', '30 - 39 %', '40 - 49 %', '50 - 59 %'], 4020, p=[0.1, 0.2, 0.2, 0.1, 0.4]),
            'Female': np.random.randint(0, 2, 4020),
            'Sa_citizen': np.random.randint(0, 2, 4020),
            'Birthyear': np.random.randint(1972, 2005, 4020),
            'Birthmonth': np.random.randint(1, 13, 4020),
            'Target': np.random.randint(0, 2, 4020)
        }
        df = pd.DataFrame(data)

    # Replicate preprocessing steps exactly as in the notebook/analysis script
    df['Survey_date'] = pd.to_datetime(df['Survey_date'], errors='coerce')
    df.rename(columns={'Female': 'Gender'}, inplace=True)

    # Store LabelEncoders for categorical columns
    label_encoders = {}
    categorical_cols = [
        'Status', 'Geography', 'Province', 'Matric', 'Degree', 'Diploma',
        'Math', 'Mathlit', 'Additional_lang', 'Home_lang', 'Science',
        'Gender', 'Highest_Education' # Highest_Education is created later
    ]

    # Convert initial categorical/binary columns and store encoders
    for col in ['Status', 'Geography', 'Province', 'Math', 'Mathlit', 'Additional_lang', 'Home_lang', 'Science']:
        le = LabelEncoder()
        df[col] = df[col].astype('category')
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le

    # Convert numeric-like floats to nullable integers where appropriate
    for col in ['Schoolquintile', 'Tenure']: # Tenure is later imputed, so keep as Int64
        df[col] = df[col].astype('Int64')

    # Convert binary columns (0/1) to category first, then encode
    for col in ['Matric', 'Degree', 'Diploma', 'Gender', 'Sa_citizen']:
        le = LabelEncoder()
        df[col] = df[col].astype('category')
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le


    # Impute missing values (mode for specific columns)
    columns_to_impute_mode = [
        'Matric', 'Degree', 'Diploma', 'Schoolquintile', 'Math', 'Mathlit',
        'Additional_lang', 'Home_lang', 'Science'
    ]
    for col in columns_to_impute_mode:
        if df[col].isnull().any():
            mode_val = df[col].mode()[0]
            df[col] = df[col].fillna(mode_val)

    # Ensure Matric, Degree, Diploma are treated as float 0/1 for get_highest_edu logic
    # They were encoded above, so we get their original value for the custom logic.
    # We will temporarily reverse the encoding for this function
    temp_df_for_edu = df.copy()
    temp_df_for_edu['Matric'] = label_encoders['Matric'].inverse_transform(temp_df_for_edu['Matric'])
    temp_df_for_edu['Degree'] = label_encoders['Degree'].inverse_transform(temp_df_for_edu['Degree'])
    temp_df_for_edu['Diploma'] = label_encoders['Diploma'].inverse_transform(temp_df_for_edu['Diploma'])

    # Convert them to float 0.0/1.0
    temp_df_for_edu['Matric'] = pd.to_numeric(temp_df_for_edu['Matric'], errors='coerce').fillna(0).astype(float)
    temp_df_for_edu['Degree'] = pd.to_numeric(temp_df_for_edu['Degree'], errors='coerce').fillna(0).astype(float)
    temp_df_for_edu['Diploma'] = pd.to_numeric(temp_df_for_edu['Diploma'], errors='coerce').fillna(0).astype(float)


    def get_highest_edu(row):
        if row['Degree'] == 1.0:
            return 'Degree'
        elif row['Diploma'] == 1.0:
            return 'Diploma'
        elif row['Matric'] == 1.0:
            return 'Matric'
        else:
            return 'None'

    df['Highest_Education'] = temp_df_for_edu.apply(get_highest_edu, axis=1)

    # Encode Highest_Education
    le_highest_edu = LabelEncoder()
    df['Highest_Education'] = le_highest_edu.fit_transform(df['Highest_Education'])
    label_encoders['Highest_Education'] = le_highest_edu

    # Impute 'Tenure' using median grouped by 'Status'
    # Ensure Status is treated as category for groupby before imputation
    original_status_le = label_encoders['Status']
    df['Status_decoded'] = original_status_le.inverse_transform(df['Status'])
    df['Tenure'] = df['Tenure'].fillna(df.groupby('Status_decoded')['Tenure'].transform('median'))
    df['Tenure'] = df['Tenure'].fillna(0).astype('Int64') # Fill any remaining NaNs with 0
    df.drop('Status_decoded', axis=1, inplace=True) # Drop the temporary column

    # Drop specified columns
    df.drop(['Person_id', 'Survey_date'], axis=1, inplace=True)

    return df, label_encoders

# --- Load Model Artifacts ---
@st.cache_resource # Cache the model loading (heavy resource)
def load_model_artifacts():
    try:
        model = joblib.load(MODEL_FILE)
        scaler_loaded = joblib.load(SCALER_FILE)
        columns = joblib.load(COLUMNS_FILE)
        return model, scaler_loaded, columns
    except FileNotFoundError as e:
        st.error(f"Error loading model artifacts: {e}. Please ensure '{MODEL_FILE}', '{SCALER_FILE}', and '{COLUMNS_FILE}' are in the same directory.")
        st.stop() # Stop the app if essential files are missing
    except Exception as e:
        st.error(f"An unexpected error occurred while loading model artifacts: {e}")
        st.stop()

model, scaler, X_columns = load_model_artifacts()

# --- Main Application Logic ---
st.set_page_config(layout="wide", page_title="Employment Status Predictor")

st.sidebar.title("Navigation")
page = st.sidebar.radio("Go to", ["Prediction", "Data Analysis"])

st.title("ðŸ‡¿ðŸ‡¦ South African Employment Status Predictor")
st.markdown("---")

# Load and preprocess data once
processed_df, label_encoders = load_and_preprocess_data(DATA_FILE)

if page == "Prediction":
    st.header("Predict Employment Status")
    st.write("Enter the individual's details to predict their employment status.")

    with st.form("prediction_form"):
        col1, col2, col3 = st.columns(3)

        with col1:
            st.subheader("Demographics")
            gender_options = label_encoders['Gender'].classes_
            gender_input = st.selectbox("Gender", gender_options, index=list(gender_options).index('Female') if 'Female' in gender_options else 0)

            sa_citizen_options = label_encoders['Sa_citizen'].classes_
            sa_citizen_input = st.selectbox("SA Citizen", sa_citizen_options, index=list(sa_citizen_options).index('Citizen') if 'Citizen' in sa_citizen_options else 0)

            birthyear_input = st.slider("Birth Year", 1970, 2005, 1995)
            birthmonth_input = st.slider("Birth Month", 1, 12, 6)

        with col2:
            st.subheader("Location & Status")
            round_input = st.slider("Round", 1, 4, 3)

            status_options = label_encoders['Status'].classes_
            status_input = st.selectbox("Status", status_options, index=list(status_options).index('unemployed') if 'unemployed' in status_options else 0)

            tenure_input = st.number_input("Tenure (days)", min_value=0, max_value=3600, value=300)

            geography_options = label_encoders['Geography'].classes_
            geography_input = st.selectbox("Geography", geography_options, index=list(geography_options).index('Urban') if 'Urban' in geography_options else 0)

            province_options = label_encoders['Province'].classes_
            province_input = st.selectbox("Province", province_options, index=list(province_options).index('Gauteng') if 'Gauteng' in province_options else 0)

        with col3:
            st.subheader("Education & Subjects")
            matric_options = label_encoders['Matric'].classes_
            matric_input = st.selectbox("Matric", matric_options, index=list(matric_options).index('Yes') if 'Yes' in matric_options else 0)

            degree_options = label_encoders['Degree'].classes_
            degree_input = st.selectbox("Degree", degree_options, index=list(degree_options).index('No') if 'No' in degree_options else 0)

            diploma_options = label_encoders['Diploma'].classes_
            diploma_input = st.selectbox("Diploma", diploma_options, index=list(diploma_options).index('No') if 'No' in diploma_options else 0)

            schoolquintile_input = st.slider("School Quintile", 0, 5, 3)

            math_options = label_encoders['Math'].classes_
            math_input = st.selectbox("Math Performance", math_options, index=list(math_options).index('40 - 49 %') if '40 - 49 %' in math_options else 0)

            mathlit_options = label_encoders['Mathlit'].classes_
            mathlit_input = st.selectbox("Math Lit Performance", mathlit_options, index=list(mathlit_options).index('40 - 49 %') if '40 - 49 %' in mathlit_options else 0)

            additional_lang_options = label_encoders['Additional_lang'].classes_
            additional_lang_input = st.selectbox("Additional Language", additional_lang_options, index=list(additional_lang_options).index('50 - 59 %') if '50 - 59 %' in additional_lang_options else 0)

            home_lang_options = label_encoders['Home_lang'].classes_
            home_lang_input = st.selectbox("Home Language", home_lang_options, index=list(home_lang_options).index('40 - 49 %') if '40 - 49 %' in home_lang_options else 0)

            science_options = label_encoders['Science'].classes_
            science_input = st.selectbox("Science Performance", science_options, index=list(science_options).index('30 - 39 %') if '30 - 39 %' in science_options else 0)

        submitted = st.form_submit_button("Predict Employment Status")

        if submitted:
            # Prepare input data for prediction
            input_data = pd.DataFrame({
                'Round': [round_input],
                'Status': [status_input],
                'Tenure': [tenure_input],
                'Geography': [geography_input],
                'Province': [province_input],
                'Matric': [matric_input],
                'Degree': [degree_input],
                'Diploma': [diploma_input],
                'Schoolquintile': [schoolquintile_input],
                'Math': [math_input],
                'Mathlit': [mathlit_input],
                'Additional_lang': [additional_lang_input],
                'Home_lang': [home_lang_input],
                'Science': [science_input],
                'Gender': [gender_input],
                'Sa_citizen': [sa_citizen_input],
                'Birthyear': [birthyear_input],
                'Birthmonth': [birthmonth_input],
                # Highest_Education will be derived after initial encoding
                'Highest_Education': [''] # Placeholder, will be computed
            })

            # Replicate preprocessing for the single input row
            # Apply LabelEncoding for categorical features
            for col, le in label_encoders.items():
                if col in input_data.columns:
                    # Handle unseen labels by coercing to a known label or error
                    # For simplicity, if a label isn't known, it will error.
                    # In a real app, you might use le.classes_.tolist() to filter input options.
                    try:
                        input_data[col] = le.transform(input_data[col])
                    except ValueError:
                        st.error(f"Input for '{col}' contains an unknown value. Please select from the dropdown options.")
                        st.stop()

            # Re-create 'Highest_Education' logic based on numerical 0/1 for Matric/Diploma/Degree
            # Assuming 0 is 'No' and 1 is 'Yes' after LabelEncoding
            def get_highest_edu_for_prediction(row):
                if row['Degree'] == 1:
                    return label_encoders['Highest_Education'].transform(['Degree'])[0]
                elif row['Diploma'] == 1:
                    return label_encoders['Highest_Education'].transform(['Diploma'])[0]
                elif row['Matric'] == 1:
                    return label_encoders['Highest_Education'].transform(['Matric'])[0]
                else:
                    return label_encoders['Highest_Education'].transform(['None'])[0]

            input_data['Highest_Education'] = input_data.apply(get_highest_edu_for_prediction, axis=1)

            # Ensure tenure is Int64 for prediction
            input_data['Tenure'] = input_data['Tenure'].astype('Int64')

            # Ensure all columns are in the same order as X_columns used in training
            input_processed = input_data[X_columns]

            # Scale numerical features (excluding 'Target' which is not in X)
            numerical_cols_for_scaling = [col for col in X_columns if col in ['Round', 'Tenure', 'Schoolquintile', 'Birthyear', 'Birthmonth']]
            input_processed[numerical_cols_for_scaling] = scaler.transform(input_processed[numerical_cols_for_scaling])


            # Make prediction
            prediction_numeric = model.predict(input_processed)[0]

            # Map prediction back to original labels
            # We need the inverse mapping for 'Target'. Assuming 0:Employed, 1:Unemployed
            # If the original `Target` column was LabelEncoded, we'd need its encoder.
            # From the PDF, it looks like Target was 0/1 int64 and then replaced for plotting.
            # So let's define a simple mapping.
            target_mapping = {0: 'Employed', 1: 'Unemployed'}
            predicted_status = target_mapping.get(prediction_numeric, "Unknown")

            st.markdown("---")
            st.subheader("Prediction Result:")
            if predicted_status == 'Employed':
                st.success(f"Based on the provided details, the individual is likely: **{predicted_status}**")
            else:
                st.warning(f"Based on the provided details, the individual is likely: **{predicted_status}**")

elif page == "Data Analysis":
    st.header("Data Analysis and Visualizations")
    st.write("Explore the distributions and relationships within the dataset.")

    st.subheader("Gender Proportion")
    gender_counts = processed_df['Gender'].replace(label_encoders['Gender'].inverse_transform(processed_df['Gender'].values)).value_counts()
    fig1, ax1 = plt.subplots(figsize=(7, 7))
    ax1.pie(gender_counts, autopct='%1.0f%%', colors=['lightcoral', 'skyblue'])
    ax1.legend(gender_counts.index, title='Gender', loc='best')
    ax1.set_title('Gender Proportion')
    st.pyplot(fig1)

    st.subheader("Province Distribution")
    province_counts = processed_df['Province'].replace(label_encoders['Province'].inverse_transform(processed_df['Province'].values)).value_counts()
    fig2, ax2 = plt.subplots(figsize=(12, 6))
    bars = province_counts.sort_values().plot(kind='barh', color='brown', ax=ax2)
    ax2.set_ylabel('Province')
    ax2.set_xlabel('Count')
    ax2.set_xticks([])
    for container in bars.containers:
        bars.bar_label(container, padding=3)
    ax2.set_title('Province Distribution')
    st.pyplot(fig2)

    st.subheader("Geography Proportion")
    geography_counts = processed_df['Geography'].replace(label_encoders['Geography'].inverse_transform(processed_df['Geography'].values)).value_counts()
    fig3, ax3 = plt.subplots(figsize=(7, 7))
    ax3.pie(geography_counts, autopct='%1.0f%%', colors=['salmon', 'lightgreen', 'lightgray'])
    ax3.legend(geography_counts.index, title='Geography', loc='best')
    ax3.set_title('Geography Proportion')
    st.pyplot(fig3)

    st.subheader("Status Distribution by Gender")
    # Need to inverse transform Status and Gender for meaningful labels
    df_plot_status_gender = processed_df.copy()
    df_plot_status_gender['Status'] = label_encoders['Status'].inverse_transform(df_plot_status_gender['Status'])
    df_plot_status_gender['Gender'] = label_encoders['Gender'].inverse_transform(df_plot_status_gender['Gender'])

    fig4, ax4 = plt.subplots(figsize=(13, 6))
    ax = sns.countplot(data=df_plot_status_gender, x='Status', hue='Gender', ax=ax4)
    ax.set_xticks([])
    for container in ax.containers:
        ax.bar_label(container, padding=3, fontsize=10)
    ax.set_title('Status Distribution by Gender')
    ax.set_xlabel('Status')
    ax.legend(title='Gender')
    st.pyplot(fig4)

    st.subheader("School Quintile Distribution")
    # Schoolquintile was converted to Int64 (nullable int) - need to convert to categories for plot if desired
    # For now, it's numerical, so we can plot as bar chart.
    schoolquintile_counts = processed_df['Schoolquintile'].value_counts(dropna=False) # Include NaN count if any
    # Inverse transform if you want labels 0,1,2,3,4,5 instead of scaled values.
    # The original notebook didn't use LabelEncoder for Schoolquintile, just Int64, so leave as is for values.
    # If the Schoolquintile values are still raw (0-5), then use them.
    # If they are scaled, the plot will look different.
    # From your PDF's plot, it seems they were scaled when plotted.
    # So, we should plot the raw value counts if possible, or scaled.
    # Given the scaling happened *after* this plot in the notebook, I will use original-like counts.
    # The processed_df here has scaled numerical values. Let's make it more visually useful.

    # To show meaningful labels, we should inverse transform if it was truly categorical:
    # df_plot_schoolquintile = processed_df.copy()
    # If Schoolquintile was label encoded, use inverse_transform.
    # Since it was Int64, it was just imputed and scaled.
    # The plot in PDF shows scaled values on Y axis.
    # So let's just plot value counts of the processed (scaled) column, which might not be intuitive.
    # A better approach here is to use the raw, pre-scaled `Schoolquintile` values for the plot.
    # Let's load fresh data for plotting, or keep track of pre-scaled values.

    # Re-loading data for plots for clarity of unscaled values, if necessary, or just plot scaled.
    # Given the prior analysis plots are for scaled values, we'll plot scaled.
    # However, the user probably wants to see actual quintile numbers for plotting.
    # Let's use the `Schoolquintile` before it was scaled, from the `load_and_preprocess_data` function, if we can.
    # For plotting, it makes more sense to plot the actual quintile numbers 0-5.
    # The `processed_df` has scaled numerical values.
    # I'll create a temporary version with unscaled values for the plot for better UX.

    raw_df_for_plots, _ = load_and_preprocess_data(DATA_FILE) # Load raw data again, as processed_df has scaled numbers
    schoolquintile_counts_raw = raw_df_for_plots['Schoolquintile'].value_counts().sort_index()

    fig5, ax5 = plt.subplots(figsize=(10, 6))
    # If the quintiles are intended as categories 0,1,2,3,4,5:
    sns.barplot(x=schoolquintile_counts_raw.index.astype(str), y=schoolquintile_counts_raw.values, palette='viridis', ax=ax5)
    ax5.set_ylabel('Count')
    ax5.set_xlabel('School Quintile')
    ax5.set_title('School Quintile Distribution')
    # Add labels to bars
    for p in ax5.patches:
        ax5.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center', xytext=(0, 5), textcoords='offset points')
    st.pyplot(fig5)

st.markdown("---")
st.markdown("Developed based on the 'GROUP_FIVE_CORRECT_FILE.pdf' analysis.")
